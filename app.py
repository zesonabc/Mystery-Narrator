import streamlit as st
import requests
import pandas as pd
import json
import re
import time
import os
import random
# å¼•å…¥ moviepy çš„é«˜çº§ç‰¹æ•ˆæ¨¡å—
from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips, ColorClip
import moviepy.video.fx.all as vfx

# ==========================================
# 1. é¡µé¢é…ç½®
# ==========================================
st.set_page_config(page_title="MysteryNarrator V13 (åŠ¨æ€è¿é•œç‰ˆ)", page_icon="ğŸ¥", layout="wide")
st.markdown("""
<style>
    .stApp { background-color: #0d0d0d; color: #c0c0c0; }
    .stButton > button { background-color: #d32f2f; color: white; width: 100%; padding: 10px; font-weight: bold; }
    .stButton > button:hover { background-color: #b71c1c; }
    .stSuccess { background-color: #1b5e20 !important; }
    img { border: 1px solid #333; border-radius: 4px; }
</style>
""", unsafe_allow_html=True)

if not os.path.exists("temp"): os.makedirs("temp")

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½
# ==========================================
def get_headers(api_key): return {"Authorization": f"Bearer {api_key}"}
def clean_json_text(text): return re.sub(r'<think>.*?</think>', '', re.sub(r'```json|```', '', text), flags=re.DOTALL).strip()

# ASR
def transcribe_audio(audio_file, api_key):
    url = "https://api.siliconflow.cn/v1/audio/transcriptions"
    temp_path = f"temp/{audio_file.name}"
    with open(temp_path, "wb") as f: f.write(audio_file.getbuffer())
    files = {'file': open(temp_path, "rb"), 'model': (None, 'FunAudioLLM/SenseVoiceSmall'), 'response_format': (None, 'verbose_json')}
    try: 
        res = requests.post(url, headers=get_headers(api_key), files=files, timeout=120).json()
        return res, temp_path
    except: return None, None

# å­—å¹•å¼ºåˆ¶åˆ‡åˆ†
def split_long_segments(segments, max_len=18):
    new_segments = []
    for seg in segments:
        text = seg['text']; start = seg['start']; end = seg['end']; duration = end - start
        if len(text) > max_len:
            parts = [text[i:i+max_len] for i in range(0, len(text), max_len)]
            part_dur = duration / len(parts)
            for i, part in enumerate(parts):
                new_segments.append({"text": part, "start": start + (i*part_dur), "end": start + ((i+1)*part_dur)})
        else: new_segments.append(seg)
    return new_segments

# è§’è‰²æå–
def extract_characters_silicon(script, model, key):
    url = "https://api.siliconflow.cn/v1/chat/completions"
    try:
        res = requests.post(url, json={"model":model, "messages":[{"role":"system","content":"æå–ã€å‰§æƒ…è§’è‰²ã€‘è¾“å‡ºJSONåˆ—è¡¨:[{'name':'xx','prompt':'...'}]"},{"role":"user","content":script}], "response_format":{"type":"json_object"}}, headers={"Authorization":f"Bearer {key}","Content-Type":"application/json"}, timeout=60)
        df = pd.DataFrame(json.loads(clean_json_text(res.json()['choices'][0]['message']['content'])))
        if not df.empty: df = df[~df['name'].str.contains('åšä¸»|æˆ‘|Host', case=False, na=False)]
        return df
    except: return None

# åˆ†é•œåˆ†æ
def analyze_segments(segments, char_names, style, res_p, model, key):
    input_data = json.dumps([{"id":i,"text":s['text']} for i,s in enumerate(segments)], ensure_ascii=False)
    char_list = ", ".join(char_names)
    sys_prompt = f"""
    æ‚¬ç–‘å¯¼æ¼”æ¨¡å¼ã€‚è§’è‰²:{char_list}ã€‚é£æ ¼:{style}ã€‚æ„å›¾:{res_p}ã€‚
    ä»»åŠ¡: ä¸ºæ¯ä¸€å¥å­—å¹•è®¾è®¡ç”»é¢ã€‚ç±»å‹: CHARACTER/SCENEã€‚Prompt: é‡è§’è‰²å†™å ä½ç¬¦[Name]ã€‚
    è¾“å‡ºJSONåˆ—è¡¨ "index", "type", "final_prompt"
    """
    try:
        res = requests.post("https://api.siliconflow.cn/v1/chat/completions", json={"model":model,"messages":[{"role":"system","content":sys_prompt},{"role":"user","content":input_data}],"response_format":{"type":"json_object"}}, headers={"Authorization":f"Bearer {key}","Content-Type":"application/json"}, timeout=120)
        result_list = json.loads(clean_json_text(res.json()['choices'][0]['message']['content']))
        if isinstance(result_list, dict): result_list = result_list.get('segments', [])
        merged = []
        for i, seg in enumerate(segments):
            vis = next((item for item in result_list if item.get('index') == i), None)
            merged.append({"start":seg['start'],"end":seg['end'],"script":seg['text'],"type":vis['type'] if vis else "SCENE","final_prompt":vis['final_prompt'] if vis else f"{style} scene"})
        return pd.DataFrame(merged)
    except: return None

# è§’è‰²æ³¨å…¥
def inject_character_prompts(shot_df, char_df):
    if shot_df is None or char_df is None: return shot_df
    char_dict = {f"[{row['name']}]": row['prompt'] for _, row in char_df.iterrows()}
    def replace(p):
        for ph in re.findall(r'\[.*?\]', p):
            if ph in char_dict: p = p.replace(ph, f"({char_dict[ph]}:1.4)")
        return p
    shot_df['final_prompt'] = shot_df['final_prompt'].apply(replace)
    return shot_df

# ç”»å›¾
def generate_image(prompt, size, key):
    try:
        res = requests.post("https://api.siliconflow.cn/v1/images/generations", json={"model":"Kwai-Kolors/Kolors","prompt":prompt,"image_size":size,"batch_size":1}, headers={"Authorization":f"Bearer {key}","Content-Type":"application/json"}, timeout=60)
        return res.json()['data'][0]['url'] if res.status_code == 200 else "Error"
    except: return "Error"

# ã€æ ¸å¿ƒå‡çº§ã€‘æ·»åŠ åŠ¨æ€è¿é•œæ•ˆæœ (Ken Burns Effect)
def add_dynamic_motion(image_clip, duration, resolution_type):
    w, h = image_clip.size
    
    # 1. å®šä¹‰è¿é•œç±»å‹ï¼ˆéšæœºé€‰æ‹©ï¼‰
    # zoom_in: æ¨è¿‘, zoom_out: æ‹‰è¿œ, pan: å¹³ç§»
    move_type = random.choice(['zoom_in', 'zoom_out', 'pan', 'pan']) # ç¨å¾®å¢åŠ å¹³ç§»çš„æ¦‚ç‡
    
    # 2. åŸºç¡€æ”¾å¤§ (ä¸ºäº†æœ‰ç©ºé—´ç§»åŠ¨ï¼Œå…ˆæŠŠå›¾æ”¾å¤§ 10%)
    zoom_ratio = 1.1
    enlarged_clip = image_clip.resize(zoom_ratio)
    ew, eh = enlarged_clip.size
    
    # è®¡ç®—æœ€å¤§å¯ç§»åŠ¨èŒƒå›´
    max_x = ew - w
    max_y = eh - h

    # 3. æ ¹æ®ç±»å‹å®šä¹‰èµ·å§‹å’Œç»“æŸçš„è£å‰ªæ¡† (Crop Box)
    if move_type == 'zoom_in':
        # ä»å…¨å›¾ -> ä¸­å¿ƒå±€éƒ¨
        x1_start, y1_start = 0, 0
        x1_end, y1_end = max_x / 2, max_y / 2
        
    elif move_type == 'zoom_out':
        # ä»ä¸­å¿ƒå±€éƒ¨ -> å…¨å›¾
        x1_start, y1_start = max_x / 2, max_y / 2
        x1_end, y1_end = 0, 0
        
    else: # 'pan' å¹³ç§»
        # éšæœºé€‰æ‹©èµ·ç‚¹å’Œç»ˆç‚¹
        x1_start = random.randint(0, int(max_x))
        y1_start = random.randint(0, int(max_y))
        
        # ç¡®ä¿ç§»åŠ¨è·ç¦»è¶³å¤Ÿå¤§ï¼Œé¿å…ç”»é¢ä¸åŠ¨
        if resolution_type == "16:9": # æ¨ªå±å€¾å‘äºæ°´å¹³ç§»
             x1_end = random.randint(0, int(max_x))
             y1_end = y1_start + random.randint(-int(max_y*0.2), int(max_y*0.2)) # Yè½´å¾®åŠ¨
        else: # ç«–å±å€¾å‘äºå‚ç›´ç§»
             x1_end = x1_start + random.randint(-int(max_x*0.2), int(max_x*0.2)) # Xè½´å¾®åŠ¨
             y1_end = random.randint(0, int(max_y))

    # è¾¹ç•Œæ£€æŸ¥
    x1_end = max(0, min(x1_end, max_x))
    y1_end = max(0, min(y1_end, max_y))

    # 4. åº”ç”¨åŠ¨æ€è£å‰ª (å…³é”®å‡½æ•°)
    # ä½¿ç”¨ lambda å‡½æ•°æ ¹æ®æ—¶é—´ t æ’å€¼è®¡ç®—å½“å‰çš„è£å‰ªåæ ‡
    moving_clip = enlarged_clip.crop(
        x1=lambda t: x1_start + (x1_end - x1_start) * (t / duration),
        y1=lambda t: y1_start + (y1_end - y1_start) * (t / duration),
        width=w, height=h
    ).set_duration(duration)

    return moving_clip

# ã€å‡çº§ã€‘æ¸²æŸ“è§†é¢‘ (åŠ å…¥è¿é•œ)
def render_video_with_motion(shot_df, image_paths, audio_path, res_type):
    clips = []
    # è·å–ç›®æ ‡åˆ†è¾¨ç‡
    target_w, target_h = (1280, 720) if res_type == "16:9" else (720, 1280)
    
    for i, row in shot_df.iterrows():
        duration = row['end'] - row['start']
        # æœ€å°ç‰‡æ®µæ—¶é•¿ï¼Œé˜²æ­¢æŠ¥é”™
        if duration < 0.5: duration = 0.5
            
        img_path = image_paths.get(i)
        
        if img_path and os.path.exists(img_path):
            # è¯»å–å›¾ç‰‡å¹¶å¼ºåˆ¶è®¾ä¸ºç›®æ ‡åˆ†è¾¨ç‡ï¼Œé˜²æ­¢å°ºå¯¸ä¸ä¸€æŠ¥é”™
            base_clip = ImageClip(img_path).resize(newsize=(target_w, target_h))
            # ã€å…³é”®ã€‘åº”ç”¨åŠ¨æ€è¿é•œ
            motion_clip = add_dynamic_motion(base_clip, duration, res_type)
            clips.append(motion_clip)
        else:
            # å¦‚æœç¼ºå›¾ï¼Œç”¨é»‘è‰²ç‰‡æ®µä»£æ›¿ï¼Œä¿è¯éŸ³é¢‘å¯¹é½
            black_clip = ColorClip(size=(target_w, target_h), color=(0,0,0)).set_duration(duration)
            clips.append(black_clip)

    if not clips: return None
    # æ‹¼æ¥
    final_video = concatenate_videoclips(clips, method="compose")
    # åŠ éŸ³é¢‘
    audio = AudioFileClip(audio_path)
    final_video = final_video.set_audio(audio)
    final_video.duration = audio.duration # å¼ºåˆ¶ä»¥éŸ³é¢‘é•¿åº¦ä¸ºå‡†
    
    output_filename = "temp/final_motion_output.mp4"
    # æ¸²æŸ“ (fps=24 çœ‹èµ·æ¥æ›´é¡ºæ»‘)
    final_video.write_videofile(output_filename, fps=24, codec="libx264", audio_codec="aac", preset="fast")
    return output_filename

# SRT
def create_srt(shot_df):
    def fmt(s): ms=int((s-int(s))*1000); m,s=divmod(int(s),60); h,m=divmod(m,60); return f"{h:02}:{m:02}:{s:02},{ms:03}"
    return "".join([f"{i+1}\n{fmt(r['start'])} --> {fmt(r['end'])}\n{r['script']}\n\n" for i,r in shot_df.iterrows()])

# ==========================================
# 3. ç•Œé¢é€»è¾‘
# ==========================================
if 'char_df' not in st.session_state: st.session_state.char_df = None
if 'shot_df' not in st.session_state: st.session_state.shot_df = None
if 'image_paths' not in st.session_state: st.session_state.image_paths = {}
if 'segments' not in st.session_state: st.session_state.segments = None 
if 'audio_path' not in st.session_state: st.session_state.audio_path = None

with st.sidebar:
    st.markdown("### ğŸ”‘ API Key"); api_key = st.text_input("SiliconFlow Key", type="password")
    st.markdown("### ğŸ•µï¸ åšä¸»å½¢è±¡"); fixed_host = st.text_area("Prompt", "(A 30-year-old Asian man, green cap, leather jacket:1.4)", height=80)
    st.markdown("### ğŸ› ï¸ è®¾ç½®"); model = st.selectbox("å¤§è„‘", ["Qwen/Qwen2.5-72B-Instruct", "deepseek-ai/DeepSeek-V3"])
    res_opt = st.selectbox("ç”»å¹…", ["16:9", "9:16"]); 
    res_type = "16:9" if res_opt == "16:9" else "9:16"
    res_str, res_prompt = {"16:9":("1280x720","Cinematic 16:9"), "9:16":("720x1280","9:16 portrait")}[res_opt]
    style = st.text_area("é£æ ¼", "Film noir, suspense thriller, low key lighting.", height=60)

st.title("ğŸ¥ MysteryNarrator V13 (åŠ¨æ€è¿é•œç‰ˆ)")
st.caption("ä¸Šä¼ å½•éŸ³ -> è‡ªåŠ¨ç”Ÿæˆå¸¦ã€æ¨æ‹‰æ‘‡ç§»ã€‘æ•ˆæœçš„æˆå“è§†é¢‘")

audio_file = st.file_uploader("ä¸Šä¼ å½•éŸ³ (MP3/WAV)", type=['mp3','wav','m4a'])

if audio_file and st.button("1. å¬å†™ & åˆ†æ"):
    if not api_key: st.error("è¯·å¡«Key")
    else:
        st.session_state.image_paths = {} # æ¸…ç©ºæ—§å›¾
        with st.spinner("å¬å†™å¹¶åˆ‡åˆ†å­—å¹•..."):
            raw_res, audio_path = transcribe_audio(audio_file, api_key)
            if raw_res:
                st.session_state.audio_path = audio_path
                raw_segments = raw_res.get('segments', [])
                clean_segs = split_long_segments(raw_segments, max_len=18)
                st.session_state.segments = clean_segs
                full_text = "".join([s['text'] for s in clean_segs])
                with st.spinner("æå–è§’è‰²..."):
                    df = extract_characters_silicon(full_text, model, api_key)
                    if df is not None:
                        host = pd.DataFrame([{"name":"åšä¸»(æˆ‘)", "prompt":fixed_host}])
                        st.session_state.char_df = pd.concat([host, df], ignore_index=True)
                        st.success(f"å°±ç»ªï¼å…± {len(clean_segs)} ä¸ªåˆ†é•œã€‚")

if st.session_state.char_df is not None:
    st.markdown("---")
    st.session_state.char_df = st.data_editor(st.session_state.char_df, num_rows="dynamic", key="c_ed")
    if st.button("2. ç”Ÿæˆåˆ†é•œè¡¨"):
        with st.spinner("å¯¼æ¼”è®¾è®¡ä¸­..."):
            char_names = st.session_state.char_df['name'].tolist()
            df = analyze_segments(st.session_state.segments, char_names, style, res_prompt, model, api_key)
            if df is not None:
                df = inject_character_prompts(df, st.session_state.char_df)
                st.session_state.shot_df = df
                st.success("åˆ†é•œå®Œæˆ")

if st.session_state.shot_df is not None:
    st.markdown("---")
    st.session_state.shot_df = st.data_editor(st.session_state.shot_df, num_rows="dynamic", key="s_ed")
    
    if st.button("ğŸš€ 3. æ¸²æŸ“åŠ¨æ€è§†é¢‘ (MP4)"):
        # 1. å…ˆç”»å›¾
        st.markdown("#### ğŸ–¼ï¸ é˜¶æ®µä¸€ï¼šç»˜åˆ¶ç”»é¢")
        bar = st.progress(0); status = st.empty(); gallery = st.columns(4)
        tot = len(st.session_state.shot_df)
        for i, r in st.session_state.shot_df.iterrows():
            status.text(f"ç»˜åˆ¶ {i+1}/{tot}: {r['script']}")
            url = generate_image(r['final_prompt'], res_str, api_key)
            if "Error" not in url:
                img_data = requests.get(url).content
                local_path = f"temp/img_{i}.jpg"
                with open(local_path, "wb") as f: f.write(img_data)
                st.session_state.image_paths[i] = local_path
                with gallery[i%4]: st.image(url, use_column_width=True)
            bar.progress((i+1)/tot); 
            if i<tot-1: time.sleep(32)
            
        # 2. ååˆæˆè§†é¢‘
        with st.spinner("ğŸ¬ é˜¶æ®µäºŒï¼šæ­£åœ¨åº”ç”¨åŠ¨æ€è¿é•œå¹¶æ¸²æŸ“ MP4 (è¯·è€å¿ƒç­‰å¾…)..."):
            # ä¼ é€’åˆ†è¾¨ç‡ç±»å‹å‚æ•°
            video_file = render_video_with_motion(st.session_state.shot_df, st.session_state.image_paths, st.session_state.audio_path, res_type)
            if video_file:
                st.success("ğŸ‰ åŠ¨æ€è§†é¢‘ç”ŸæˆæˆåŠŸï¼")
                st.video(video_file)
                with open(video_file, "rb") as f: vb = f.read()
                st.download_button("â¬‡ï¸ ä¸‹è½½æœ€ç»ˆè§†é¢‘ (MP4)", vb, "final_motion_story.mp4", "video/mp4")
                st.download_button("â¬‡ï¸ ä¸‹è½½é…å¥—å­—å¹• (SRT)", create_srt(st.session_state.shot_df), "subtitle.srt", "text/plain")
            else: st.error("è§†é¢‘æ¸²æŸ“å¤±è´¥")
